Metadata-Version: 2.4
Name: l3m-backend
Version: 0.1.0
Summary: A lightweight tool-calling system for local LLMs using llama-cpp-python
License: MIT
Project-URL: Homepage, https://github.com/mo/l3m-backend
Project-URL: Repository, https://github.com/mo/l3m-backend
Keywords: llm,tool-calling,llama-cpp,ai,chatbot
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: pydantic>=2.0
Requires-Dist: requests>=2.25
Provides-Extra: llm
Requires-Dist: llama-cpp-python>=0.2.0; extra == "llm"
Provides-Extra: repl
Requires-Dist: prompt_toolkit>=3.0; extra == "repl"
Provides-Extra: mcp
Requires-Dist: mcp[cli]>=1.0; extra == "mcp"
Provides-Extra: dev
Requires-Dist: pytest>=7.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0; extra == "dev"
Requires-Dist: pytest-mock>=3.10; extra == "dev"
Provides-Extra: all
Requires-Dist: l3m-backend[dev,llm,mcp,repl]; extra == "all"

# l3m-backend

A lightweight tool-calling system for local LLMs using llama-cpp-python.

Uses a contract-based approach to tool calling that works with any model capable of generating JSON, without requiring native function-calling support.

## Installation

```bash
# Basic installation
pip install git+https://github.com/mo/l3m-backend.git

# With LLM support (llama-cpp-python)
pip install 'git+https://github.com/mo/l3m-backend.git[llm]'

# With enhanced REPL (prompt_toolkit)
pip install 'git+https://github.com/mo/l3m-backend.git[repl]'

# Full installation
pip install 'git+https://github.com/mo/l3m-backend.git[llm,repl]'
```

## Quick Start

```bash
# Initialize configuration directories
l3m-init

# Download a model
l3m-download --preset llama3.2-1b

# Start chatting
l3m-chat
```

## CLI Commands

| Command | Description |
|---------|-------------|
| `l3m-init` | Initialize configuration and directories |
| `l3m-chat` | Interactive chat REPL with tool calling |
| `l3m-tools` | Tool management (list, info, create) |
| `l3m-download` | Download GGUF models from Hugging Face |
| `l3m-completion` | Generate shell completion scripts |

### Examples

```bash
# Initialize l3m directories
l3m-init

# List available models
l3m-chat --list

# Chat with specific model
l3m-chat Llama-3.2-1B-Instruct-Q4_K_M.gguf

# List available tools
l3m-tools list
l3m-tools info get_weather

# Create a custom tool
l3m-tools create my_tool

# Enable bash completion
source <(l3m-completion bash)
```

## Python API

```python
from l3m_backend import ToolRegistry, tool_output

registry = ToolRegistry()

@registry.register(aliases=["weather", "w"])
@tool_output(llm_format="{location}: {temperature}°C")
def get_weather(location: str) -> dict:
    return {"location": location, "temperature": 22}

# Use with ChatEngine (requires llama-cpp-python)
from l3m_backend import ChatEngine
from l3m_backend.tools import registry  # Pre-loaded with built-in tools

engine = ChatEngine("./model.gguf", registry)
response = engine.chat("What's the weather in Paris?")
```

## Built-in Tools

- **calculate** - Evaluate mathematical expressions
- **get_time** - Get current date and time
- **get_weather** - Get weather for a location
- **wikipedia** - Fetch Wikipedia summaries
- **define_word** - Look up word definitions
- **unit_convert** / **currency_convert** - Unit and currency conversion
- **web_search** - Search the web
- **run_python** - Execute Python code
- **shell_cmd** - Execute shell commands
- **read_file** / **write_file** - File operations
- **http_request** - Make HTTP requests
- **note** / **todo** / **reminder** / **timer** - Productivity tools
- **random_number** / **uuid_generate** / **hash_text** / **base64_encode** / **json_format** - Utilities

Run `l3m-tools list` for the full list with descriptions.

## REPL Commands

The chat REPL provides several types of commands:

### Slash Commands (/)

Pure commands that don't modify conversation history:

| Command | Description |
|---------|-------------|
| `/clear` | Clear conversation history |
| `/tools` | List available tools |
| `/system` | Show system prompt |
| `/schema` | Show tool schema (OpenAI format) |
| `/contract` | Show full system message with tools contract |
| `/history` | Show conversation history |
| `/undo` | Remove last user+assistant exchange |
| `/pop [n]` | Remove last n messages |
| `/context` | Show context usage estimate |
| `/model` | Show model info |
| `/config` | Show current configuration |
| `/help` | Show help message |
| `/quit` | Exit (also `/exit`, `/q`) |

### Shell Commands (!)

Run shell commands directly from the REPL:

```
!ls -la
!git status
!python --version
```

### Magic Commands (%)

Magic commands add their input and output to the conversation history, making them visible to the LLM:

| Command | Description |
|---------|-------------|
| `%!<cmd>` | Run shell command (adds output to history) |
| `%tool <name> [json]` | Invoke tool directly |
| `%load <file>` | Load file content into conversation |
| `%time` | Report current time |
| `%save <file>` | Save conversation to file |
| `%edit-response` | Edit last assistant response in `$VISUAL` editor |

### Session Management

Session commands for persistent conversations:

| Command | Description |
|---------|-------------|
| `/session` | Show current session info |
| `/sessions [query]` | List or search sessions |
| `/session-save [title]` | Save session (auto-generates title if needed) |
| `/session-load <id>` | Load session as context |
| `/session-title [title]` | Set or generate session title |
| `/session-tag <tag>` | Add tag to session |

### Keyboard Shortcuts

| Shortcut | Action |
|----------|--------|
| `Tab` | Auto-complete commands |
| `Ctrl+R` | Search command history |
| `Ctrl+C` | Cancel current input |
| `Ctrl+D` (twice) | Exit REPL |

## User-Defined Tools

Create custom tools in `~/.l3m/tools/`. Each tool is a Python module in its own subdirectory.

### Creating a Tool

```bash
# Create a basic tool scaffold
l3m-tools create my_tool

# Create a wrapper for a shell command
l3m-tools create docker-ps --wrap "docker ps"
```

Tools are automatically loaded when `l3m-chat` starts.

### Tool Structure

```python
# ~/.l3m/tools/my_tool/__init__.py
from typing import Any
from l3m_backend.tools import registry
from l3m_backend.core import tool_output

@registry.register(aliases=["mt"])
@tool_output(llm_format="{result}")
def my_tool(input: str) -> dict[str, Any]:
    """Process the input and return a result."""
    return {"result": f"Processed: {input}"}
```

## Model Presets

```bash
l3m-download --presets  # Show all presets

# Available presets:
#   llama3.2-1b   - Llama 3.2 1B Instruct (~0.8GB)
#   llama3.2-3b   - Llama 3.2 3B Instruct (~2GB)
#   mistral-7b    - Mistral 7B Instruct v0.3 (~4.4GB)
#   granite-8b    - IBM Granite 3.1 8B Instruct (~5GB)
#   qwen2.5-3b    - Qwen 2.5 3B Instruct (~2GB)
#   phi3-mini     - Microsoft Phi 3.5 Mini (~2.4GB)
```

## Default Paths

- **Configuration**: `~/.l3m/config.json`
- **Models**: `~/.l3m/models/`
- **User tools**: `~/.l3m/tools/`
- **Sessions**: `~/.l3m/sessions/`
- **Chat history**: `~/.l3m/prompt_history`

Run `l3m-init` to create these directories and a default configuration file.

## Documentation

### Man Pages

Man pages are installed automatically with `pip install`. View them with:

```bash
man l3m-chat
man l3m-tools
man l3m-download
man l3m-completion
```

Source location: `docs/man/`

### TLDR Pages

Quick reference pages in TLDR format are included:

```bash
# View TLDR pages (if using tldr client with custom pages)
tldr l3m-chat

# Or read directly
cat docs/tldr/l3m-chat.md
```

Source location: `docs/tldr/`

Installation locations (via pip):
- Man pages: `<prefix>/share/man/man1/`
- TLDR pages: `<prefix>/share/tldr/pages/`

## Testing

```bash
# Run unit tests
python -m pytest tests/ -v

# Run E2E tests with Hermes 8B (requires model download)
python scripts/e2e_test_hermes.py
```

The E2E test suite uses obfuscated test tools (`get_flumbuster`, `calculate_zorbix`) to validate tool calling without training data contamination.

## How It Works

The system uses a contract-based approach where tools respond with JSON:

```json
{"type": "tool_call", "name": "get_weather", "arguments": {"location": "Paris"}}
```

```json
{"type": "final", "content": "The weather in Paris is 22°C and sunny."}
```

This works with any model capable of generating JSON, without requiring native function-calling support.

## License

MIT
